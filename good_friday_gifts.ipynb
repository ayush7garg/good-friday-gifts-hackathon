{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "good_friday_gifts.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMw/9DJJbW7+5tJ45ZGjHpP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayush7garg/good-friday-gifts-hackathon/blob/master/good_friday_gifts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fulYvg_OoEP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Importing Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PW4Srp2NQybT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('train.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knwQRAcJRsMV",
        "colab_type": "code",
        "outputId": "1f0379a4-5ee6-46be-d6fd-de3894360684",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "df.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20279, 17)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X-sK0hPFSj1q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df['volumes'] = df['volumes'].fillna(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJZjTs-Qx4km",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = df.drop(\"price\",axis=1)\n",
        "y_train = df[\"price\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "At-ZHUrX3FfA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cor = df.corr()\n",
        "cor_target = abs(cor[\"price\"])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7lmKa1aUS_TX",
        "colab_type": "code",
        "outputId": "fa0e74a0-d12f-4361-b68f-5a6824fda875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        }
      },
      "source": [
        "cor_target"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "gift_type        0.137145\n",
              "gift_category    0.286888\n",
              "gift_cluster     0.282881\n",
              "lsg_1            0.147712\n",
              "lsg_2            0.185142\n",
              "lsg_3            0.006178\n",
              "lsg_4            0.222104\n",
              "lsg_5            0.525773\n",
              "lsg_6            0.019769\n",
              "is_discounted    0.157463\n",
              "volumes          0.132591\n",
              "price            1.000000\n",
              "Name: price, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MU5VmEUWTBPY",
        "colab_type": "code",
        "outputId": "f908c01d-70e5-4af7-c221-350570b6f82a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 446
        }
      },
      "source": [
        "cor"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gift_type</th>\n",
              "      <th>gift_category</th>\n",
              "      <th>gift_cluster</th>\n",
              "      <th>lsg_1</th>\n",
              "      <th>lsg_2</th>\n",
              "      <th>lsg_3</th>\n",
              "      <th>lsg_4</th>\n",
              "      <th>lsg_5</th>\n",
              "      <th>lsg_6</th>\n",
              "      <th>is_discounted</th>\n",
              "      <th>volumes</th>\n",
              "      <th>price</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>gift_type</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.075328</td>\n",
              "      <td>-0.123263</td>\n",
              "      <td>0.369368</td>\n",
              "      <td>-0.063898</td>\n",
              "      <td>0.534393</td>\n",
              "      <td>0.055803</td>\n",
              "      <td>-0.073652</td>\n",
              "      <td>0.062589</td>\n",
              "      <td>-0.027933</td>\n",
              "      <td>-0.100975</td>\n",
              "      <td>0.137145</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gift_category</th>\n",
              "      <td>0.075328</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.138297</td>\n",
              "      <td>0.003494</td>\n",
              "      <td>-0.020970</td>\n",
              "      <td>-0.069802</td>\n",
              "      <td>0.053803</td>\n",
              "      <td>-0.339733</td>\n",
              "      <td>0.085429</td>\n",
              "      <td>0.011843</td>\n",
              "      <td>-0.232432</td>\n",
              "      <td>0.286888</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gift_cluster</th>\n",
              "      <td>-0.123263</td>\n",
              "      <td>-0.138297</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.106383</td>\n",
              "      <td>0.109072</td>\n",
              "      <td>0.067285</td>\n",
              "      <td>0.068613</td>\n",
              "      <td>0.146533</td>\n",
              "      <td>0.025014</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>0.233788</td>\n",
              "      <td>-0.282881</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lsg_1</th>\n",
              "      <td>0.369368</td>\n",
              "      <td>0.003494</td>\n",
              "      <td>-0.106383</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.162152</td>\n",
              "      <td>0.524086</td>\n",
              "      <td>-0.126679</td>\n",
              "      <td>-0.125548</td>\n",
              "      <td>-0.033314</td>\n",
              "      <td>-0.193836</td>\n",
              "      <td>0.335478</td>\n",
              "      <td>0.147712</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lsg_2</th>\n",
              "      <td>-0.063898</td>\n",
              "      <td>-0.020970</td>\n",
              "      <td>0.109072</td>\n",
              "      <td>-0.162152</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.037239</td>\n",
              "      <td>0.022979</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.039292</td>\n",
              "      <td>0.035532</td>\n",
              "      <td>0.035085</td>\n",
              "      <td>-0.185142</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lsg_3</th>\n",
              "      <td>0.534393</td>\n",
              "      <td>-0.069802</td>\n",
              "      <td>0.067285</td>\n",
              "      <td>0.524086</td>\n",
              "      <td>-0.037239</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.080235</td>\n",
              "      <td>-0.002776</td>\n",
              "      <td>0.067858</td>\n",
              "      <td>-0.120458</td>\n",
              "      <td>0.303429</td>\n",
              "      <td>0.006178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lsg_4</th>\n",
              "      <td>0.055803</td>\n",
              "      <td>0.053803</td>\n",
              "      <td>0.068613</td>\n",
              "      <td>-0.126679</td>\n",
              "      <td>0.022979</td>\n",
              "      <td>0.080235</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.272718</td>\n",
              "      <td>0.371960</td>\n",
              "      <td>0.201302</td>\n",
              "      <td>-0.321000</td>\n",
              "      <td>-0.222104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lsg_5</th>\n",
              "      <td>-0.073652</td>\n",
              "      <td>-0.339733</td>\n",
              "      <td>0.146533</td>\n",
              "      <td>-0.125548</td>\n",
              "      <td>0.040390</td>\n",
              "      <td>-0.002776</td>\n",
              "      <td>0.272718</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.163077</td>\n",
              "      <td>0.094139</td>\n",
              "      <td>0.054567</td>\n",
              "      <td>-0.525773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>lsg_6</th>\n",
              "      <td>0.062589</td>\n",
              "      <td>0.085429</td>\n",
              "      <td>0.025014</td>\n",
              "      <td>-0.033314</td>\n",
              "      <td>-0.039292</td>\n",
              "      <td>0.067858</td>\n",
              "      <td>0.371960</td>\n",
              "      <td>0.163077</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.122089</td>\n",
              "      <td>-0.161461</td>\n",
              "      <td>-0.019769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>is_discounted</th>\n",
              "      <td>-0.027933</td>\n",
              "      <td>0.011843</td>\n",
              "      <td>0.063864</td>\n",
              "      <td>-0.193836</td>\n",
              "      <td>0.035532</td>\n",
              "      <td>-0.120458</td>\n",
              "      <td>0.201302</td>\n",
              "      <td>0.094139</td>\n",
              "      <td>0.122089</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.262461</td>\n",
              "      <td>-0.157463</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>volumes</th>\n",
              "      <td>-0.100975</td>\n",
              "      <td>-0.232432</td>\n",
              "      <td>0.233788</td>\n",
              "      <td>0.335478</td>\n",
              "      <td>0.035085</td>\n",
              "      <td>0.303429</td>\n",
              "      <td>-0.321000</td>\n",
              "      <td>0.054567</td>\n",
              "      <td>-0.161461</td>\n",
              "      <td>-0.262461</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>-0.132591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>price</th>\n",
              "      <td>0.137145</td>\n",
              "      <td>0.286888</td>\n",
              "      <td>-0.282881</td>\n",
              "      <td>0.147712</td>\n",
              "      <td>-0.185142</td>\n",
              "      <td>0.006178</td>\n",
              "      <td>-0.222104</td>\n",
              "      <td>-0.525773</td>\n",
              "      <td>-0.019769</td>\n",
              "      <td>-0.157463</td>\n",
              "      <td>-0.132591</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               gift_type  gift_category  ...   volumes     price\n",
              "gift_type       1.000000       0.075328  ... -0.100975  0.137145\n",
              "gift_category   0.075328       1.000000  ... -0.232432  0.286888\n",
              "gift_cluster   -0.123263      -0.138297  ...  0.233788 -0.282881\n",
              "lsg_1           0.369368       0.003494  ...  0.335478  0.147712\n",
              "lsg_2          -0.063898      -0.020970  ...  0.035085 -0.185142\n",
              "lsg_3           0.534393      -0.069802  ...  0.303429  0.006178\n",
              "lsg_4           0.055803       0.053803  ... -0.321000 -0.222104\n",
              "lsg_5          -0.073652      -0.339733  ...  0.054567 -0.525773\n",
              "lsg_6           0.062589       0.085429  ... -0.161461 -0.019769\n",
              "is_discounted  -0.027933       0.011843  ... -0.262461 -0.157463\n",
              "volumes        -0.100975      -0.232432  ...  1.000000 -0.132591\n",
              "price           0.137145       0.286888  ... -0.132591  1.000000\n",
              "\n",
              "[12 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PooKFeO9TJrn",
        "colab_type": "code",
        "outputId": "a049bb1a-136c-43e2-9220-ca2d4f48fa42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "cor.columns"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['gift_type', 'gift_category', 'gift_cluster', 'lsg_1', 'lsg_2', 'lsg_3',\n",
              "       'lsg_4', 'lsg_5', 'lsg_6', 'is_discounted', 'volumes', 'price'],\n",
              "      dtype='object')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40yNNhYCTt4T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cor_target = pd.DataFrame(cor_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD-5c5lSUx3t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cor_target = cor_target.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCzgLWvZVCsE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "to_drop = [i for i in list(cor_target.columns) if cor_target[i]['price']<0.1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UC7j7VrpVQpS",
        "colab_type": "code",
        "outputId": "dab6c44c-e2b6-47f2-83f9-d1fb9c0db562",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "to_drop"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lsg_3', 'lsg_6']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeyb4pbIWRAA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.drop(columns=to_drop)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeVG9ZB0Wnlz",
        "colab_type": "code",
        "outputId": "e5e720df-95ea-4338-aa63-fccb6db79518",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        }
      },
      "source": [
        "x_train.head(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gift_id</th>\n",
              "      <th>gift_type</th>\n",
              "      <th>gift_category</th>\n",
              "      <th>gift_cluster</th>\n",
              "      <th>instock_date</th>\n",
              "      <th>stock_update_date</th>\n",
              "      <th>lsg_1</th>\n",
              "      <th>lsg_2</th>\n",
              "      <th>lsg_4</th>\n",
              "      <th>lsg_5</th>\n",
              "      <th>uk_date1</th>\n",
              "      <th>uk_date2</th>\n",
              "      <th>is_discounted</th>\n",
              "      <th>volumes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>GF_11156</td>\n",
              "      <td>61</td>\n",
              "      <td>534</td>\n",
              "      <td>3942</td>\n",
              "      <td>2014-02-21 05:07:06.000</td>\n",
              "      <td>2016-11-09 15:49:51.000</td>\n",
              "      <td>3377</td>\n",
              "      <td>5221</td>\n",
              "      <td>1912</td>\n",
              "      <td>10</td>\n",
              "      <td>2014-02-24 08:07:06.000</td>\n",
              "      <td>2014-02-24 07:07:06.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>GF_11157</td>\n",
              "      <td>61</td>\n",
              "      <td>534</td>\n",
              "      <td>3942</td>\n",
              "      <td>2014-02-21 06:07:06.000</td>\n",
              "      <td>2016-11-11 13:49:51.000</td>\n",
              "      <td>3377</td>\n",
              "      <td>5221</td>\n",
              "      <td>1912</td>\n",
              "      <td>10</td>\n",
              "      <td>2014-02-22 07:07:06.000</td>\n",
              "      <td>2014-02-24 06:07:06.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>GF_15689</td>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-02-21 09:30:21.000</td>\n",
              "      <td>2016-03-24 14:46:18.000</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>2016-01-26 00:04:45.000</td>\n",
              "      <td>2016-03-18 02:00:00.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GF_11155</td>\n",
              "      <td>61</td>\n",
              "      <td>534</td>\n",
              "      <td>3942</td>\n",
              "      <td>2014-02-22 05:07:06.000</td>\n",
              "      <td>2016-11-10 16:49:51.000</td>\n",
              "      <td>3377</td>\n",
              "      <td>5221</td>\n",
              "      <td>1912</td>\n",
              "      <td>10</td>\n",
              "      <td>2016-11-07 13:49:51.000</td>\n",
              "      <td>2016-11-06 04:00:00.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>GF_11158</td>\n",
              "      <td>61</td>\n",
              "      <td>534</td>\n",
              "      <td>3942</td>\n",
              "      <td>2014-02-22 07:07:06.000</td>\n",
              "      <td>2016-11-10 13:49:51.000</td>\n",
              "      <td>3377</td>\n",
              "      <td>5221</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>2016-11-07 15:49:51.000</td>\n",
              "      <td>2016-11-06 01:00:00.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>GF_15686</td>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-02-23 07:30:21.000</td>\n",
              "      <td>2016-03-23 15:46:18.000</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>2014-02-25 07:30:21.000</td>\n",
              "      <td>2014-02-24 08:30:21.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>GF_15690</td>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-02-23 10:30:21.000</td>\n",
              "      <td>2016-03-23 18:46:18.000</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>2016-01-25 02:04:45.000</td>\n",
              "      <td>2016-03-16 02:00:00.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>GF_15685</td>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-02-24 10:30:21.000</td>\n",
              "      <td>2016-03-24 17:46:18.000</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>2016-01-22 03:04:45.000</td>\n",
              "      <td>2016-03-17 03:00:00.000</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>GF_11159</td>\n",
              "      <td>61</td>\n",
              "      <td>534</td>\n",
              "      <td>3942</td>\n",
              "      <td>2014-02-25 07:07:06.000</td>\n",
              "      <td>2016-11-10 14:49:51.000</td>\n",
              "      <td>3377</td>\n",
              "      <td>5221</td>\n",
              "      <td>1912</td>\n",
              "      <td>10</td>\n",
              "      <td>2016-11-07 16:49:51.000</td>\n",
              "      <td>2015-04-08 12:48:27.025</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>GF_15688</td>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-02-25 08:30:21.000</td>\n",
              "      <td>2016-03-24 16:46:18.000</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>2016-01-23 03:04:45.000</td>\n",
              "      <td>2016-03-11 04:00:00.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>GF_15687</td>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-02-25 10:30:21.000</td>\n",
              "      <td>2016-03-22 18:46:18.000</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>2016-01-23 23:04:45.000</td>\n",
              "      <td>2016-03-20 04:00:00.000</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>GF_19007</td>\n",
              "      <td>1136</td>\n",
              "      <td>890</td>\n",
              "      <td>0</td>\n",
              "      <td>2014-08-31 04:40:59.000</td>\n",
              "      <td>2016-08-01 17:52:10.000</td>\n",
              "      <td>8143</td>\n",
              "      <td>3350</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>2014-09-02 03:40:59.000</td>\n",
              "      <td>2014-08-28 01:21:25.696</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>GF_519</td>\n",
              "      <td>168</td>\n",
              "      <td>403</td>\n",
              "      <td>1846</td>\n",
              "      <td>2014-10-15 22:59:24.000</td>\n",
              "      <td>2017-04-03 15:40:33.000</td>\n",
              "      <td>3799</td>\n",
              "      <td>3358</td>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>2017-03-31 12:40:33.000</td>\n",
              "      <td>2017-03-26 12:25:37.484</td>\n",
              "      <td>0</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>GF_522</td>\n",
              "      <td>168</td>\n",
              "      <td>403</td>\n",
              "      <td>1846</td>\n",
              "      <td>2014-10-16 21:59:24.000</td>\n",
              "      <td>2017-04-02 16:40:33.000</td>\n",
              "      <td>3799</td>\n",
              "      <td>3358</td>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>2017-03-06 17:31:38.000</td>\n",
              "      <td>2017-02-14 19:48:35.300</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>GF_524</td>\n",
              "      <td>168</td>\n",
              "      <td>403</td>\n",
              "      <td>1846</td>\n",
              "      <td>2014-10-18 23:59:24.000</td>\n",
              "      <td>2017-04-02 15:40:33.000</td>\n",
              "      <td>3799</td>\n",
              "      <td>3358</td>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>2014-11-18 13:43:12.000</td>\n",
              "      <td>2014-11-15 15:03:20.063</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "     gift_id  gift_type  ...  is_discounted  volumes\n",
              "0   GF_11156         61  ...              0      0.0\n",
              "1   GF_11157         61  ...              1      0.0\n",
              "2   GF_15689        584  ...              1      0.0\n",
              "3   GF_11155         61  ...              0      0.0\n",
              "4   GF_11158         61  ...              1      0.0\n",
              "5   GF_15686        584  ...              1      0.0\n",
              "6   GF_15690        584  ...              1      0.0\n",
              "7   GF_15685        584  ...              0      0.0\n",
              "8   GF_11159         61  ...              0      0.0\n",
              "9   GF_15688        584  ...              1      0.0\n",
              "10  GF_15687        584  ...              1      0.0\n",
              "11  GF_19007       1136  ...              0      0.0\n",
              "12    GF_519        168  ...              0     25.0\n",
              "13    GF_522        168  ...              0     26.0\n",
              "14    GF_524        168  ...              0     26.0\n",
              "\n",
              "[15 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7L6OMK_XkiC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.drop(columns=['instock_date','stock_update_date','uk_date1','uk_date2','gift_id'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iLiyUH6ajvEv",
        "colab_type": "code",
        "outputId": "951d0dfc-02f3-4abd-cdb1-68c9a719c347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 520
        }
      },
      "source": [
        "x_train.head(15)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>gift_type</th>\n",
              "      <th>gift_category</th>\n",
              "      <th>gift_cluster</th>\n",
              "      <th>lsg_1</th>\n",
              "      <th>lsg_2</th>\n",
              "      <th>lsg_4</th>\n",
              "      <th>lsg_5</th>\n",
              "      <th>is_discounted</th>\n",
              "      <th>volumes</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>61</td>\n",
              "      <td>534</td>\n",
              "      <td>3942</td>\n",
              "      <td>3377</td>\n",
              "      <td>5221</td>\n",
              "      <td>1912</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>61</td>\n",
              "      <td>534</td>\n",
              "      <td>3942</td>\n",
              "      <td>3377</td>\n",
              "      <td>5221</td>\n",
              "      <td>1912</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>61</td>\n",
              "      <td>534</td>\n",
              "      <td>3942</td>\n",
              "      <td>3377</td>\n",
              "      <td>5221</td>\n",
              "      <td>1912</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>61</td>\n",
              "      <td>534</td>\n",
              "      <td>3942</td>\n",
              "      <td>3377</td>\n",
              "      <td>5221</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>61</td>\n",
              "      <td>534</td>\n",
              "      <td>3942</td>\n",
              "      <td>3377</td>\n",
              "      <td>5221</td>\n",
              "      <td>1912</td>\n",
              "      <td>10</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>584</td>\n",
              "      <td>262</td>\n",
              "      <td>0</td>\n",
              "      <td>5290</td>\n",
              "      <td>1579</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>1</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>1136</td>\n",
              "      <td>890</td>\n",
              "      <td>0</td>\n",
              "      <td>8143</td>\n",
              "      <td>3350</td>\n",
              "      <td>1912</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>168</td>\n",
              "      <td>403</td>\n",
              "      <td>1846</td>\n",
              "      <td>3799</td>\n",
              "      <td>3358</td>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>25.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>168</td>\n",
              "      <td>403</td>\n",
              "      <td>1846</td>\n",
              "      <td>3799</td>\n",
              "      <td>3358</td>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>168</td>\n",
              "      <td>403</td>\n",
              "      <td>1846</td>\n",
              "      <td>3799</td>\n",
              "      <td>3358</td>\n",
              "      <td>51</td>\n",
              "      <td>9</td>\n",
              "      <td>0</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    gift_type  gift_category  gift_cluster  ...  lsg_5  is_discounted  volumes\n",
              "0          61            534          3942  ...     10              0      0.0\n",
              "1          61            534          3942  ...     10              1      0.0\n",
              "2         584            262             0  ...      9              1      0.0\n",
              "3          61            534          3942  ...     10              0      0.0\n",
              "4          61            534          3942  ...      9              1      0.0\n",
              "5         584            262             0  ...      9              1      0.0\n",
              "6         584            262             0  ...      9              1      0.0\n",
              "7         584            262             0  ...      9              0      0.0\n",
              "8          61            534          3942  ...     10              0      0.0\n",
              "9         584            262             0  ...      9              1      0.0\n",
              "10        584            262             0  ...      9              1      0.0\n",
              "11       1136            890             0  ...      9              0      0.0\n",
              "12        168            403          1846  ...      9              0     25.0\n",
              "13        168            403          1846  ...      9              0     26.0\n",
              "14        168            403          1846  ...      9              0     26.0\n",
              "\n",
              "[15 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxaQ0ZmnkTYm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "sc_X = StandardScaler()\n",
        "x_train = sc_X.fit_transform(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GxOzXtckja4",
        "colab_type": "code",
        "outputId": "07e40a68-48f3-4891-fdcd-6c3173502767",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 568
        }
      },
      "source": [
        "x_train[0:15][:]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-1.74342697,  0.59483247,  0.25133272, -0.71676497,  0.45425411,\n",
              "         0.47941927,  0.57348482, -0.54599014, -0.64149666],\n",
              "       [-1.74342697,  0.59483247,  0.25133272, -0.71676497,  0.45425411,\n",
              "         0.47941927,  0.57348482,  1.8315349 , -0.64149666],\n",
              "       [-0.39967037, -0.56225996, -1.30001286, -0.00909843, -1.14675209,\n",
              "         0.47941927,  0.14783155,  1.8315349 , -0.64149666],\n",
              "       [-1.74342697,  0.59483247,  0.25133272, -0.71676497,  0.45425411,\n",
              "         0.47941927,  0.57348482, -0.54599014, -0.64149666],\n",
              "       [-1.74342697,  0.59483247,  0.25133272, -0.71676497,  0.45425411,\n",
              "         0.47941927,  0.14783155,  1.8315349 , -0.64149666],\n",
              "       [-0.39967037, -0.56225996, -1.30001286, -0.00909843, -1.14675209,\n",
              "         0.47941927,  0.14783155,  1.8315349 , -0.64149666],\n",
              "       [-0.39967037, -0.56225996, -1.30001286, -0.00909843, -1.14675209,\n",
              "         0.47941927,  0.14783155,  1.8315349 , -0.64149666],\n",
              "       [-0.39967037, -0.56225996, -1.30001286, -0.00909843, -1.14675209,\n",
              "         0.47941927,  0.14783155, -0.54599014, -0.64149666],\n",
              "       [-1.74342697,  0.59483247,  0.25133272, -0.71676497,  0.45425411,\n",
              "         0.47941927,  0.57348482, -0.54599014, -0.64149666],\n",
              "       [-0.39967037, -0.56225996, -1.30001286, -0.00909843, -1.14675209,\n",
              "         0.47941927,  0.14783155,  1.8315349 , -0.64149666],\n",
              "       [-0.39967037, -0.56225996, -1.30001286, -0.00909843, -1.14675209,\n",
              "         0.47941927,  0.14783155,  1.8315349 , -0.64149666],\n",
              "       [ 1.01859663,  2.10926227, -1.30001286,  1.0462976 , -0.36822876,\n",
              "         0.47941927,  0.14783155, -0.54599014, -0.64149666],\n",
              "       [-1.46850927,  0.03755634, -0.57353292, -0.56065661, -0.36471199,\n",
              "        -3.35226544,  0.14783155, -0.54599014,  2.2209011 ],\n",
              "       [-1.46850927,  0.03755634, -0.57353292, -0.56065661, -0.36471199,\n",
              "        -3.35226544,  0.14783155, -0.54599014,  2.33539701],\n",
              "       [-1.46850927,  0.03755634, -0.57353292, -0.56065661, -0.36471199,\n",
              "        -3.35226544,  0.14783155, -0.54599014,  2.33539701]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56-l5Tt2k5Wx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = pd.DataFrame(x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KiX1fF3hli-V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "x_tr,x_test,y_tr,y_test = train_test_split(x_train,y_train,test_size=0.25,random_state=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "enRB4Lk6l0jb",
        "colab_type": "code",
        "outputId": "5870e80f-1996-424d-80c1-7185b241bb1a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#SVM model\n",
        "from sklearn import svm\n",
        "model = svm.SVR()\n",
        "model.fit(x_tr,y_tr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVR(C=1.0, cache_size=200, coef0=0.0, degree=3, epsilon=0.1, gamma='scale',\n",
              "    kernel='rbf', max_iter=-1, shrinking=True, tol=0.001, verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kbqz8C4TmBIX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_pred = model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npei9zUtmPE4",
        "colab_type": "code",
        "outputId": "6d1ea6c8-2c32-462d-cb07-b22ef1a93af5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.metrics import mean_absolute_error\n",
        "100 - np.sqrt(mean_absolute_error(y_test, y_pred))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91.41400498463125"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LKOw9SITmvDr",
        "colab_type": "code",
        "outputId": "fd489913-5573-4bb5-b400-ca322400c450",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "#KNN model\n",
        "from sklearn import neighbors\n",
        "model_knn = neighbors.KNeighborsRegressor(n_neighbors = 3,weights='distance',algorithm='ball_tree',p=1)\n",
        "model_knn.fit(x_tr, y_tr)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsRegressor(algorithm='ball_tree', leaf_size=30, metric='minkowski',\n",
              "                    metric_params=None, n_jobs=None, n_neighbors=3, p=1,\n",
              "                    weights='distance')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fOvLEGDjnEwD",
        "colab_type": "code",
        "outputId": "4e936f35-42d4-4956-e2f7-15e9f24cc314",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_pred_knn = model_knn.predict(x_test)\n",
        "100 - np.sqrt(mean_absolute_error(y_test, y_pred_knn))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93.70662051060398"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOyD0BWLnWsH",
        "colab_type": "code",
        "outputId": "f386d251-7aa8-4b55-862f-65439f9d9da3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#Decision Trees\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "tree = DecisionTreeRegressor(criterion='mae',max_depth=55000)\n",
        "tree.fit(x_tr,y_tr)\n",
        "y_pred_tree=tree.predict(x_test)\n",
        "100 - np.sqrt(mean_absolute_error(y_test, y_pred_tree))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "93.43006061289026"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZoaMdelnmVU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test = pd.read_csv('test.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9yNQ-eLNoDie",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df_test['volumes'] = df_test['volumes'].fillna(0)\n",
        "x_test1 = df_test.drop(columns=['lsg_3', 'lsg_6','instock_date','stock_update_date','uk_date1','uk_date2','gift_id'])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EPezIDZQo7aB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test1.head(15)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruoeCaw4o_jm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_test1 = sc_X.fit_transform(x_test1)\n",
        "x_test1 = pd.DataFrame(x_test1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MY771UCupf46",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = model_knn.predict(x_test1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HL-qCOERpnUj",
        "colab_type": "code",
        "outputId": "3e8faaca-2c00-49df-80ad-91b35f4c9456",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "predictions[0:15]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 38.58685561,  46.02684074,  44.4973813 ,  97.01688266,\n",
              "        43.67818953, 234.12455094,  83.18400405,  28.8803106 ,\n",
              "        62.51333333,  62.51333333, 214.79829459,  47.8299361 ,\n",
              "       177.48420613, 137.18682866, 164.76214408])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpJUO3jLqQ-Q",
        "colab_type": "code",
        "outputId": "147b2dd0-7f1f-4b02-aeb9-26627a37952d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "from sklearn.linear_model import SGDRegressor\n",
        "model_sgd=SGDRegressor(loss=\"epsilon_insensitive\",alpha=0.0001,l1_ratio=0.2,penalty='elasticnet',fit_intercept\n",
        "                   =True,max_iter=2000,learning_rate='invscaling',warm_start=False)\n",
        "\n",
        "model_sgd.fit(x_tr,y_tr)\n",
        "y_pred_sgd = model_sgd.predict(x_test)\n",
        "100 - np.sqrt(mean_absolute_error(y_test, y_pred_sgd))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90.68936147793804"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OyFGxqPSwFsG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n",
        "regressor = RandomForestRegressor(n_estimators=250,random_state=3,criterion=\"mae\")\n",
        "regressor.fit(x_tr,y_tr)\n",
        "y_pred_r = regressor.predict(x_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3zW8ufazs5k",
        "colab_type": "code",
        "outputId": "dbb24576-e6c5-4db9-a740-99c715f88613",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_pred_r = np.around(y_pred_r,decimals=2)\n",
        "100 - np.sqrt(mean_absolute_error(y_test, y_pred_r))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "94.00664108493304"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cChY8aByXict",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictions = regressor.predict(x_test1)\n",
        "predictions = np.around(predictions,decimals=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xM7qPOBepqb0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = []\n",
        "submission.append(df_test[\"gift_id\"])\n",
        "submission.append(predictions)\n",
        "submission = np.asarray(submission)\n",
        "submission = np.transpose(submission)\n",
        "np.savetxt('prediction.csv',submission,fmt='%s,%f',delimiter=',',header=\"gift_id,price\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-em8UXewX6wy",
        "colab_type": "code",
        "outputId": "0864c31c-6ecd-41a2-e96c-032d12f32475",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "predictions[0:10]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 92.82, 100.94,  62.41,  80.21,  49.99, 191.41,  93.37,  55.02,\n",
              "       119.48, 119.48])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uk_s7k5WYZyd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Neural Networks aproach\n",
        "x_neu = df.drop(\"price\",axis=1)\n",
        "x_neu = x_neu.drop(columns=['instock_date','stock_update_date','uk_date1','uk_date2','gift_id'])\n",
        "y_neu = df[\"price\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-ZmjJC2cpzI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_neu.head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qrxk7gYAcuZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_neu_a = np.asarray(x_neu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DyNLHNBZc8e1",
        "colab_type": "code",
        "outputId": "1f69ade2-1f0a-4163-8ef1-9fef318ba8b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "x_neu_a[0:5]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[6.100e+01, 5.340e+02, 3.942e+03, 3.377e+03, 5.221e+03, 5.040e+02,\n",
              "        1.912e+03, 1.000e+01, 5.540e+02, 0.000e+00, 0.000e+00],\n",
              "       [6.100e+01, 5.340e+02, 3.942e+03, 3.377e+03, 5.221e+03, 5.040e+02,\n",
              "        1.912e+03, 1.000e+01, 5.540e+02, 1.000e+00, 0.000e+00],\n",
              "       [5.840e+02, 2.620e+02, 0.000e+00, 5.290e+03, 1.579e+03, 3.203e+03,\n",
              "        1.912e+03, 9.000e+00, 1.578e+03, 1.000e+00, 0.000e+00],\n",
              "       [6.100e+01, 5.340e+02, 3.942e+03, 3.377e+03, 5.221e+03, 5.040e+02,\n",
              "        1.912e+03, 1.000e+01, 5.540e+02, 0.000e+00, 0.000e+00],\n",
              "       [6.100e+01, 5.340e+02, 3.942e+03, 3.377e+03, 5.221e+03, 5.040e+02,\n",
              "        1.912e+03, 9.000e+00, 5.540e+02, 1.000e+00, 0.000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GcCWHhhJdAQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_neu_t = x_neu_a.T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMQeay4ueCxu",
        "colab_type": "code",
        "outputId": "31281770-6886-4565-b985-7176729f020d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_neu_t.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11, 20279)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQ4enL3GeFUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_neu_t = np.atleast_2d(y_neu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "514fqLxPer7s",
        "colab_type": "code",
        "outputId": "d8becfb0-8379-4c1e-eeae-0cfb75aa4f76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_neu_t.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 20279)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLvmbNGhfvPS",
        "colab_type": "text"
      },
      "source": [
        "Initializing parameters for the layers of neural networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QuBAKFNe1tk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Arguments:\n",
        "    layer_dims -- python array (list) containing the dimensions of each layer in our network\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector of shape (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    np.random.seed(3)\n",
        "    parameters = {}\n",
        "    L = len(layer_dims)            # number of layers in the network\n",
        "\n",
        "    for l in range(1, L):\n",
        "        \n",
        "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])*0.01\n",
        "        parameters['b' + str(l)] = np.zeros([layer_dims[l],1])\n",
        "        \n",
        "        \n",
        "        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n",
        "        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n",
        "\n",
        "        \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0UFQwCIVf4RN",
        "colab_type": "text"
      },
      "source": [
        "Defining the forward propagation steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tVKER93ftTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implement the linear part of a layer's forward propagation.\n",
        "\n",
        "    Arguments:\n",
        "    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- the input of the activation function, also called pre-activation parameter \n",
        "    cache -- a python tuple containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    Z = np.dot(W,A) + b\n",
        "    \n",
        "    assert(Z.shape == (W.shape[0], A.shape[1]))\n",
        "    cache = (A, W, b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aK9KXkbjgFCj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation for the LINEAR->ACTIVATION layer\n",
        "\n",
        "    Arguments:\n",
        "    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n",
        "    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n",
        "    b -- bias vector, numpy array of shape (size of the current layer, 1)\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "\n",
        "    Returns:\n",
        "    A -- the output of the activation function, also called the post-activation value \n",
        "    cache -- a python tuple containing \"linear_cache\" and \"activation_cache\";\n",
        "             stored for computing the backward pass efficiently\n",
        "    \"\"\"\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = 1/(1+np.exp(-Z)),Z\n",
        "    \n",
        "    elif activation == \"relu\":\n",
        "        Z, linear_cache = linear_forward(A_prev, W, b)\n",
        "        A, activation_cache = np.maximum(0,Z),Z\n",
        "    \n",
        "    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n",
        "    cache = (linear_cache, activation_cache)\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2cYz1atRgWna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_forward(X, parameters):\n",
        "    \"\"\"\n",
        "    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (input size, number of examples)\n",
        "    parameters -- output of initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL -- last post-activation value\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n",
        "    \"\"\"\n",
        "\n",
        "    caches = []\n",
        "    A = X\n",
        "    L = len(parameters) // 2                  # number of layers in the neural network\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        A_prev = A \n",
        "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], \"relu\")\n",
        "        caches.append(cache)\n",
        "    \n",
        "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"relu\")\n",
        "    caches.append(cache)\n",
        "    \n",
        "    assert(AL.shape == (1,X.shape[1]))\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ipm71ggmoE8B",
        "colab_type": "text"
      },
      "source": [
        "Computing cost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcp6J_JFgr1m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implement the cost function defined by equation (7).\n",
        "\n",
        "    Arguments:\n",
        "    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n",
        "    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "    m = Y.shape[1]\n",
        "\n",
        "    # Compute loss from aL and y.\n",
        "    cost = (1/m)*np.sum(abs(AL-Y))\n",
        "    \n",
        "    cost = np.squeeze(cost)      # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n",
        "    assert(cost.shape == ())\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qq2hOke2oMrQ",
        "colab_type": "text"
      },
      "source": [
        "Defining Backward Propagation Steps"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmwGEuv0g-JA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
        "\n",
        "    Arguments:\n",
        "    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n",
        "    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    A_prev, W, b = cache\n",
        "    m = A_prev.shape[1]\n",
        "\n",
        "    dW = (1/m)*np.dot(dZ,A_prev.T)\n",
        "    db = (1/m)*np.sum(dZ,axis=1,keepdims=True)\n",
        "    dA_prev = np.dot(W.T,dZ)\n",
        "    \n",
        "    assert (dA_prev.shape == A_prev.shape)\n",
        "    assert (dW.shape == W.shape)\n",
        "    assert (db.shape == b.shape)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqisNtIwhkFK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
        "    \n",
        "    Arguments:\n",
        "    dA -- post-activation gradient for current layer l \n",
        "    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n",
        "    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
        "    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n",
        "    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n",
        "    \"\"\"\n",
        "    linear_cache, activation_cache = cache\n",
        "    Z = activation_cache\n",
        "    gZ = Z\n",
        "    gZ[gZ>0] = 1\n",
        "    gZ[gZ<0] = 0\n",
        "\n",
        "    if activation == \"relu\":\n",
        "        dZ = dA*gZ\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "        \n",
        "    elif activation == \"sigmoid\":\n",
        "        dZ = dA*(1/(1+np.exp(-Z)))*(1 - (1/(1+np.exp(-Z))))\n",
        "        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3jq6aQAOjxUV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
        "    \n",
        "    Arguments:\n",
        "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
        "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
        "    caches -- list of caches containing:\n",
        "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
        "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
        "    \n",
        "    Returns:\n",
        "    grads -- A dictionary with the gradients\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads = {}\n",
        "    L = len(caches) # the number of layers\n",
        "    m = AL.shape[1]\n",
        "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
        "    \n",
        "    # Initializing the backpropagation\n",
        "    dAL = -(np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
        "    \n",
        "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n",
        "\n",
        "    current_cache = caches[L-1]\n",
        "    grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, \"relu\")\n",
        "\n",
        "    \n",
        "    # Loop from l=L-2 to l=0\n",
        "    for l in reversed(range(L-1)):\n",
        "        # lth layer: (RELU -> LINEAR) gradients.\n",
        "        current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\"+str(l+1)], current_cache, \"relu\")\n",
        "        grads[\"dA\" + str(l)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecaDDnoRlVyM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def update_parameters(parameters, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Update parameters using gradient descent\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters \n",
        "    grads -- python dictionary containing your gradients, output of L_model_backward\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "                  parameters[\"W\" + str(l)] = ... \n",
        "                  parameters[\"b\" + str(l)] = ...\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural network\n",
        "\n",
        "\n",
        "    for l in range(L):\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate*grads[\"dW\"+str(l+1)]\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate*grads[\"db\"+str(l+1)]\n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b4I1Jn1um3kt",
        "colab_type": "code",
        "outputId": "aab1feaa-b359-4e7a-f67f-6eb6831fcd21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "x_neu_t.shape\n",
        "y_neu_t.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 20279)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXfUpfZ5nNtq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers_dims = [11, 20, 7, 5, 1] #  4-layer model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3o4Y5MnnX1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):#lr was 0.009\n",
        "    \"\"\"\n",
        "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
        "    \n",
        "    Arguments:\n",
        "    X -- data, numpy array of shape (num_px * num_px * 3, number of examples)\n",
        "    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n",
        "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
        "    learning_rate -- learning rate of the gradient descent update rule\n",
        "    num_iterations -- number of iterations of the optimization loop\n",
        "    print_cost -- if True, it prints the cost every 100 steps\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
        "    \"\"\"\n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []                         # keep track of cost\n",
        "    \n",
        "    # Parameters initialization. ( 1 line of code)\n",
        "    ### START CODE HERE ###\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        ### START CODE HERE ### ( 1 line of code)\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        ### END CODE HERE ###\n",
        "        \n",
        "        # Compute cost.\n",
        "        ### START CODE HERE ### ( 1 line of code)\n",
        "        cost = compute_cost(AL, Y)\n",
        "        ### END CODE HERE ###\n",
        "    \n",
        "        # Backward propagation.\n",
        "        ### START CODE HERE ### ( 1 line of code)\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        ### END CODE HERE ###\n",
        " \n",
        "        # Update parameters.\n",
        "        ### START CODE HERE ### ( 1 line of code)\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "        ### END CODE HERE ###\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per hundreds)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r9OTetwdofT9",
        "colab_type": "code",
        "outputId": "69829330-6df9-4d30-93ea-d3e01d52c268",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 435
        }
      },
      "source": [
        "parameters = L_layer_model(x_neu_t, y_neu_t, layers_dims, num_iterations = 2500, print_cost = True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: RuntimeWarning: invalid value encountered in greater\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:19: RuntimeWarning: invalid value encountered in less\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Cost after iteration 0: nan\n",
            "Cost after iteration 100: nan\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-55-edb83fbbdbc2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_neu_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_neu_t\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m2500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-54-18215096b7a3>\u001b[0m in \u001b[0;36mL_layer_model\u001b[0;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;31m# Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m### START CODE HERE ### ( 1 line of code)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;31m### END CODE HERE ###\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-46-e0558a465b72>\u001b[0m in \u001b[0;36mL_model_forward\u001b[0;34m(X, parameters)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0mAL\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_activation_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"W\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"b\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mcaches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-45-3dfedf808682>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[0;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"relu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mZ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlinear_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m         \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation_cache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaximum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-44-175a673c6b2a>\u001b[0m in \u001b[0;36mlinear_forward\u001b[0;34m(A, W, b)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32massert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ArgjuoedVoN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}